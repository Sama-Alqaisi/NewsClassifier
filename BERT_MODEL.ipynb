{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "BWHcyi_wKHKF",
        "bTlxp4p5KMj9",
        "1y6YLML_KPxo",
        "XKgsDnNcKWXC",
        "KU8NxMJbubLa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEwKgHcnQG1E",
        "outputId": "ea4ec7b9-08ec-4afa-a9b3-7cfe4a6b7f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWHcyi_wKHKF"
      },
      "source": [
        "# **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddi51CWox8CK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpZCgTzMAiw8"
      },
      "outputs": [],
      "source": [
        "news_train=pd.read_excel('/content/drive/MyDrive/News_train.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSchvg9YQ-QE",
        "outputId": "59efb2c0-080a-4e4d-e0a8-d5b35a7eab52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "news_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTlxp4p5KMj9"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y6YLML_KPxo"
      },
      "source": [
        "### **re**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG4FLKz8FT8C",
        "outputId": "1bf75f3e-edf9-408a-b463-d3c7ddbbad84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.10/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarabic"
      ],
      "metadata": {
        "id": "nOeGYNbeFXkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llK2Eci2AyS1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from pyarabic.araby import tokenize\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    # Normalize Arabic text\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn') # remove diacritics\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub('/', ' ', text)\n",
        "    text = re.sub('-', ' ', text)\n",
        "    text = re.sub('_', ' ', text)\n",
        "    text = re.sub(' و ', ' ', text)\n",
        "    text = re.sub(\"'\", ' ', text)\n",
        "    text = re.sub(\"``\", ' ', text)\n",
        "    text = re.sub('\"', ' ', text)\n",
        "    text = re.sub('%', ' ', text)\n",
        "    text = re.sub('»', ' ', text)\n",
        "    text = re.sub('«', ' ', text)\n",
        "    text = re.sub(r'\\bال(\\w+)\\b', r'\\1', text)\n",
        "    text = re.sub(r'\\bلل(\\w+)\\b', r'\\1', text)\n",
        "    text = re.sub(r'\\bبال(\\w+)\\b', r'\\1', text)\n",
        "    text = re.sub(r'[A-Za-z0-9]', r'', text)#remove english characters\n",
        "    text = re.sub(r'[0-9]', r'', text)#remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', r'', text)#remove punctuation\n",
        "\n",
        "    words = tokenize(text)\n",
        "\n",
        "    normalized_text = ' '.join(words)\n",
        "\n",
        "    return normalized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHJtqAx-C65d"
      },
      "outputs": [],
      "source": [
        "news_train['News'] = news_train['News'].apply(normalize_arabic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNnNttwgC62o",
        "outputId": "1a49f213-aaf0-40b0-be25-7631b4ff460b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       اشتباك حريري عون اتهامات لباسيل تمسك ثلث معطل ...\n",
              "1                               عون حريري اصبح غريب اطوار\n",
              "2       وزير خارجيه امريكي ندرس سحب كامل قواتنا من افغ...\n",
              "3       افغانستان استعدادات حثيثه لاجتماع تركيا وكابل ...\n",
              "4                   اندبندنت مفاوضات سريه كادت تنقذ قذافي\n",
              "                              ...                        \n",
              "4995                اوروبا تبدا احصاء خساير فيضانات مدمره\n",
              "4996    قتل متظاهر رصاص خلال احتجاجات علي شح مياه في م...\n",
              "4997    وسايل اعلام ايرانيه تتحدث عن اندلاع احتجاجات ف...\n",
              "4998           مفاوضات افغانيه تتواصل في عاصمه قطريه دوحه\n",
              "4999    تعليق مفاوضات افغانيه في دوحه موقتا لمزيد من م...\n",
              "Name: News, Length: 5000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "news_train['News']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiSx6tOrhv3K",
        "outputId": "9688c066-7ed9-4d59-ed27-53795df594e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "news_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgsDnNcKWXC"
      },
      "source": [
        "### **nltk**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kztx2gmGEmTQ",
        "outputId": "60a9da14-0820-45c9-c884-4b5ecce4a7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZmYopVYFvmM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pyarabic.araby import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import ISRIStemmer\n",
        "\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization for Arabic text using PyArabic's tokenize function\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    # Get Arabic stop words from NLTK\n",
        "    stop_words_arabic = set(stopwords.words('arabic'))\n",
        "\n",
        "    # Remove stop words\n",
        "    arabic_tokens = [token for token in tokens if token not in stop_words_arabic]\n",
        "\n",
        "    # Stemming Arabic text\n",
        "    arabic_tokens = [stemmer.stem(token) for token in arabic_tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    processed_text = ' '.join(arabic_tokens)\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eIzGzoYZ9HO"
      },
      "outputs": [],
      "source": [
        "preprocessed_documents_news_train = [preprocess_text(doc) for doc in news_train['News']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfRwf66caP9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b6e9c0-63a0-48ab-80be-78a5ba581372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(preprocessed_documents_news_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8NxMJbubLa"
      },
      "source": [
        "# **Splitting the data into training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmuPmwOHqSfL"
      },
      "outputs": [],
      "source": [
        "news_train['processed_text'] = preprocessed_documents_news_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRtdxYItZm5K",
        "outputId": "2f9e1521-727f-43d9-f37f-3d2f1ce40353"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                 شبك حرر عون تهم اسل تمس ثلث عطل بقء حكم\n",
              "1                                    عون حرر اصبح غرب طور\n",
              "2                     وزر خرج امر درس سحب كمل قوت غنس حلل\n",
              "3       غنس عدد حثث جمع ترك كبل تهم طلب ركز علي حرب بد...\n",
              "4                            اندبندنت فاض سره كدت نقذ قذف\n",
              "                              ...                        \n",
              "4995                             ورب تبد حصء خسير فيض دمر\n",
              "4996      قتل ظاهر رصص خلل حجج علي شح ياه طقه جنب غرب اير\n",
              "4997                          سيل علم يرن حدث دلع حجج شرع\n",
              "4998                              فاض فغن وصل عصم قطر دوح\n",
              "4999                          علق فاض فغن دوح وقت زيد شار\n",
              "Name: processed_text, Length: 5000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "news_train['processed_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUTqRljYZB_e",
        "outputId": "110b4880-f901-410c-8332-502ea03a87e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Type', 'News', 'processed_text'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "news_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7VAT3txjvER"
      },
      "outputs": [],
      "source": [
        "y=news_train['Type']\n",
        "X=news_train.drop(columns=['Type', 'News'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn9UnuJhTj0O",
        "outputId": "8207e3de-09fe-4d61-a038-872a2dfeae27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       politics\n",
              "1       politics\n",
              "2       politics\n",
              "3       politics\n",
              "4       politics\n",
              "          ...   \n",
              "4995    politics\n",
              "4996    politics\n",
              "4997    politics\n",
              "4998    politics\n",
              "4999    politics\n",
              "Name: Type, Length: 5000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfwoItxEjelq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oUkNNUOb1fL",
        "outputId": "bda70062-e7a9-49a4-9bc1-35c662f6fcd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4250"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "jm50zAHGL0Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bert Embeddings and Bert Model**"
      ],
      "metadata": {
        "id": "ULtEZr_Bnad8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code1:"
      ],
      "metadata": {
        "id": "k8l_BNwPFA4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9jLhST2J8wQ",
        "outputId": "5b6a2764-f7ed-400c-deee-fb803598ad3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "BUBFzB5bLNrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "256a62a67c8c408aaab522534b5724ff",
            "1cf54bf93f134ce996a06aabc09cca5f",
            "b26c2d7f78954b0685748dddbd1585e0",
            "3f75ec59f8274b86b207b2e9f3ccd0ba",
            "b0b84904bb6442e6a28f1c19fb317ffa",
            "ebd8de54e0cb49ef8c3748fa3fccb942",
            "a913399edfee4fe4a7ec34011ae125f0",
            "3464d7e9f4a443a3a753375abb44987c",
            "478ba7a505a1440da6995c6727441b7e",
            "05057a05ac09449e8f5b3d979fa44dde",
            "4c8c3c147c3d4130b135a691112bf635",
            "c9d9c594a14d426c97ed7aca4f29f7d1",
            "6c8e3ffc0d6045c2a5648106b0de6b9e",
            "cf15c4922e3742de962b05e564296483",
            "a0d1beba01fb42c29aefb01d8cd7dad5",
            "36f0f7fe5fec4fa3a417191740e6b092",
            "2c945d4dec0c47d7a545258965302e71",
            "e8c766724c194b3cbb1166d0a1dc26aa",
            "ad8996070fa345b9bac5de10b78ee0de",
            "ba0ef6a8c69744699687caa74f23ed72",
            "24fe8983df0f41c79a8827fd7e332c98",
            "4cac8f5b1dd6455287dfa7486e8dbb7d",
            "cb6e8a627500453098274852c74d3ecd",
            "af5906dc3a234a2c9cd10e8144d98ed3",
            "69abc813f2b443ce9a5c1a88d64e9ae4",
            "89e2e1d346c14595bac89f1d1a348987",
            "39b7f194ba6b4050a5bdd203c775e239",
            "b0c1faa674bf4444bea4c0ac93326694",
            "f12d09e81ff6415b87c3b1bf65d0152b",
            "77e78150561e4e5bbffad6e2dd598542",
            "24299a08f92e4871a5977f856e614045",
            "daa6d3e7549e447c95c6ec78910cb950",
            "97bf1dd86ae341e485d528b09e363c22",
            "9b4279fe1ead4317a71580f1f5702358",
            "2f4b90d1ba5440789d5a342246f1c787",
            "febb47461f0f4c51b67543e239ed8195",
            "567c3daf385943c2a0a1053907502b92",
            "5994c8d8b69b489396e4ea31bd475e5a",
            "2d71300133a64d27bcb22f6e4f60cd20",
            "1adb1661afc143f4b944c1c6f3847b41",
            "7e987090595847a197fc67ddcc70188a",
            "447998a1df0340aaaa5c6811b7d93022",
            "729004ed288a420a8268bd7114abb1bb",
            "adfa9a0f73a741ff84b9c4f59e71e2ca",
            "74befde856db42af9df64031ec275c9d",
            "52536798354b45cc8e5ec7ec80467abf",
            "f7397bbbd6b64aee810a1eb68d34d7af",
            "0b8274c7036c4917bdaee2d7a72e2831",
            "7f79a42f4c7043c4b082dde7063ae8a8",
            "2f5ab622a6dd4cbaa4cedddd6dc2a163",
            "85402b1e4fa844b89e46d4cdfe19f62f",
            "06f9e6089f3c435e9ba20c84e115a798",
            "586ad26703e5497c98d83c06c3ddad85",
            "51edf18ca5cd4fb289764a97f1c35e33",
            "78c48073957442979e76064f6379ba84"
          ]
        },
        "id": "mU196-I7fJXN",
        "outputId": "563f9003-f571-45bb-888f-cbd00349e987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "256a62a67c8c408aaab522534b5724ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9d9c594a14d426c97ed7aca4f29f7d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb6e8a627500453098274852c74d3ecd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b4279fe1ead4317a71580f1f5702358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74befde856db42af9df64031ec275c9d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "label_mapping = {'economic': 0, 'politics': 1, 'sport': 2, 'tech': 3}\n",
        "\n",
        "num_class = len(set(label_mapping.values()))\n",
        "\n",
        "# Encode the categorical labels into numerical format using the mapping\n",
        "y_train_encoded_manual = np.array([label_mapping[label] for label in y_train])\n",
        "y_test_encoded_manual = np.array([label_mapping[label] for label in y_test])\n",
        "\n",
        "y_train_one_hot = to_categorical(y_train_encoded_manual, num_classes=num_class)\n",
        "y_test_one_hot = to_categorical(y_test_encoded_manual, num_classes=num_class)\n"
      ],
      "metadata": {
        "id": "WANZgPoLfTa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze bert parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = nn.Linear(bert_model.config.hidden_size, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "zzUGXfVdJZ_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokens = tokenizer(X_train['processed_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "X_test_tokens = tokenizer(X_test['processed_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)"
      ],
      "metadata": {
        "id": "WBbC6uP9gVCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to tensor\n",
        "y_train_tensor = torch.tensor(y_train_one_hot).to(device)\n",
        "y_test_tensor = torch.tensor(y_test_one_hot).to(device)\n",
        "\n",
        "num_labels = 4\n",
        "classifier_model = BertClassifier(model)\n",
        "classifier_model = classifier_model.to(device)\n",
        "\n",
        "optimizer = AdamW(classifier_model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx9smsyhgaoO",
        "outputId": "9ca82888-550d-45ed-b65b-7043a52a4047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size & epoch\n",
        "batch_size = 8\n",
        "num_epochs=8"
      ],
      "metadata": {
        "id": "YsepuGfLgIZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    classifier_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tokens['input_ids'], X_train_tokens['attention_mask'], y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = classifier_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels.argmax(dim=1))\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss = {loss.item()}')"
      ],
      "metadata": {
        "id": "jyza3d3cgDnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a92b8cb-74f0-4f54-b6db-a162f0bd18e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 1.950777292251587\n",
            "Epoch 2: Loss = 0.8874014019966125\n",
            "Epoch 3: Loss = 2.868354558944702\n",
            "Epoch 4: Loss = 1.3071318864822388\n",
            "Epoch 5: Loss = 1.0836200714111328\n",
            "Epoch 6: Loss = 1.0743759870529175\n",
            "Epoch 7: Loss = 0.8181892037391663\n",
            "Epoch 8: Loss = 0.34849026799201965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = classifier_model(input_ids=X_test_tokens['input_ids'].to(device),\n",
        "                                   attention_mask=X_test_tokens['attention_mask'].to(device))\n",
        "\n",
        "test_loss = nn.CrossEntropyLoss()(test_outputs, y_test_tensor.argmax(dim=1))\n",
        "\n",
        "#accuracy for model\n",
        "_, predicted_labels = torch.max(test_outputs, 1)\n",
        "correct = (predicted_labels == y_test_tensor.argmax(dim=1)).sum().item()\n",
        "total = y_test_tensor.size(0)\n",
        "accuracy = correct / total\n",
        "print(f'test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "JJTRPvh4MCPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0151ae3-4e36-4b9b-d11a-aa3bd22e45c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Accuracy: 65.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predicted_labels_cpu = predicted_labels.cpu().numpy()\n",
        "y_test_cpu = y_test_tensor.argmax(dim=1).cpu().numpy()\n",
        "report = classification_report(y_test_cpu, predicted_labels_cpu)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "r5xIN5yrlGaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3455b519-98ad-4763-ed5a-692daf3175ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       134\n",
            "           1       0.65      1.00      0.79       488\n",
            "           2       0.00      0.00      0.00       120\n",
            "           3       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.65       750\n",
            "   macro avg       0.16      0.25      0.20       750\n",
            "weighted avg       0.42      0.65      0.51       750\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code 2:"
      ],
      "metadata": {
        "id": "An3o9xaCEQhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "#Tokenize and encode the data using the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTjBIOV___HX",
        "outputId": "5ccb2897-5a24-402b-fa33-97ae2166d51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
            "\n",
            "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning:\n",
            "\n",
            "\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_len = 128\n",
        "\n",
        "# Tokenize and encode the sentences\n",
        "X_train_encoded = tokenizer.batch_encode_plus(\n",
        "    X_train['processed_text'].tolist(),  # Convert the specific column to list\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_len,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "X_test_encoded = tokenizer.batch_encode_plus(\n",
        "    X_test['processed_text'].tolist(),  # Convert the specific column to list\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_len,\n",
        "    return_tensors='tf'\n",
        ")\n"
      ],
      "metadata": {
        "id": "q-GbjrcABTrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intialize the model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dspEW7P7EBrN",
        "outputId": "a240355e-30b2-45cb-b9ff-e4d0b8b937af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.11.0 transformers==4.26.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdO6YBv8Ezq2",
        "outputId": "a726be34-3452-40b2-db49-a703af257e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: transformers==4.26.0 in /usr/local/lib/python3.10/dist-packages (4.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (0.23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (4.66.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.43.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (2024.6.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct imports\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYq-09BhEk7h",
        "outputId": "4b83437b-dc57-4d2d-d4aa-4df3d1e0dac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "obCijoFUIhC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels to numerical format\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "6VdhRVT7ImcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the model\n",
        "history = model.fit(\n",
        "\t[X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
        "\ty_train_encoded,\n",
        "\tvalidation_data=(\n",
        "\t[X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']], y_test_encoded),\n",
        "\tbatch_size=16, \tepochs=3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qXKPj2nF-nB",
        "outputId": "4ef1ea0a-47b6-4998-e2f2-823e77a422ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "266/266 [==============================] - 4624s 17s/step - loss: 0.9255 - accuracy: 0.6489 - val_loss: 0.6991 - val_accuracy: 0.7387\n",
            "Epoch 2/3\n",
            "266/266 [==============================] - 4480s 17s/step - loss: 0.6064 - accuracy: 0.7779 - val_loss: 0.4398 - val_accuracy: 0.8360\n",
            "Epoch 3/3\n",
            "266/266 [==============================] - 4581s 17s/step - loss: 0.4331 - accuracy: 0.8555 - val_loss: 0.3918 - val_accuracy: 0.8627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming your model is named 'model'\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
        "    y_test_encoded\n",
        ")\n",
        "\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
        "\n",
        "# Get predictions\n",
        "predictions = model.predict([X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "LYxGAMP9GZ44",
        "outputId": "b8ea724f-48fe-475b-d0eb-a607252dfbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 233s 10s/step - loss: 0.3918 - accuracy: 0.8627\n",
            "Test loss: 0.39175188541412354, Test accuracy: 0.862666666507721\n",
            "24/24 [==============================] - 215s 9s/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-894dac45aa5a>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert one-hot encoded labels back to original labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_test_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test_encoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpredictions_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-894dac45aa5a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert one-hot encoded labels back to original labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_test_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test_encoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpredictions_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}